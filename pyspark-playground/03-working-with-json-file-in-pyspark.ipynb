{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76f9dda7-656d-40ca-b65d-762df879d54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f885d86b-09b8-48c4-8f24-f80099ffd0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session is ready to handle JSON Files\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"Json Handling in Pyspark\") \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "print(\"Spark session is ready to handle JSON Files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e46ce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93516005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------------+-------------+-------------+-------------+------+\n",
      "|code|message|         restaurants|results_found|results_shown|results_start|status|\n",
      "+----+-------+--------------------+-------------+-------------+-------------+------+\n",
      "|NULL|   NULL|[{{{308322}, b90e...|        17151|           20|            1|  NULL|\n",
      "|NULL|   NULL|[{{{18017612}, b9...|         4748|           20|            1|  NULL|\n",
      "|NULL|   NULL|[{{{18313566}, b9...|        13786|           20|            1|  NULL|\n",
      "|NULL|   NULL|[{{{18353121}, b9...|        10224|           20|            1|  NULL|\n",
      "|NULL|   NULL|[{{{18354483}, b9...|         7039|           20|            1|  NULL|\n",
      "+----+-------+--------------------+-------------+-------------+-------------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Basic JSON Read\n",
    "df = spark.read.json('./dataset/json_data/file1.json')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8e93887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common JSON Reading Options\n",
    "\n",
    "# Multiline: Set multiline=True if each json records spread across multiple lines.\n",
    "# InferSchema: JSON file often include structure data, so PySpark will infer the schema \\\n",
    "# automatically. You can manually specify schema if needed.\n",
    "# SamplingRatio: When inferring schema, PySpark reads a sample of the data to determine types. samplingRatio allows you to set the fraction of rows to sample (between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f135bce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n",
      "|age|    name|salary|\n",
      "+---+--------+------+\n",
      "| 20|  Manish| 20000|\n",
      "| 25|  Nikita| 21000|\n",
      "| 16|  Pritam| 22000|\n",
      "| 35|Prantosh| 25000|\n",
      "| 67|  Vikash| 40000|\n",
      "+---+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('multiline', True) \\\n",
    "                .option('sampleRatio', 0.5) \\\n",
    "                .json('./dataset/json_data/multi_line_correct.json')\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a2c4ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a DataFrame to JSON File\n",
    "\n",
    "# df.write.json(\"dataset/output_data/json_data/test_write_1\")\n",
    "\n",
    "# df.write.mode(\"overwrite\") \\\n",
    "#         .option(\"compression\", \"gzip\") \\\n",
    "#         .json(\"dataset/output_data/json_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "085fed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning Output Files:\n",
    "\n",
    "# For large JSON data, partitioning output files by specific columns can be useful. \\\n",
    "# Partitioning splits data based on the column values, creating sub-folders named after each partition column, making large datasets easier to manage and access.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e463adf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
