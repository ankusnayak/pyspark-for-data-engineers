{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72c8e522-da85-4ad3-a502-5a5f364347fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Using cached pyspark-4.0.1.tar.gz (434.2 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting py4j==0.10.9.9 (from pyspark)\n",
      "  Using cached py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Using cached py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml): started\n",
      "  Building wheel for pyspark (pyproject.toml): still running...\n",
      "  Building wheel for pyspark (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-4.0.1-py2.py3-none-any.whl size=434813901 sha256=9624eb151c05ddda68a529f067a99b9300f2de4c9220de4145f5831c190f2ad3\n",
      "  Stored in directory: c:\\users\\ankus\\appdata\\local\\pip\\cache\\wheels\\00\\e3\\92\\8594f4cee2c9fd4ad82fe85e4bf2559ab8ea84ef19b1dd3d15\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   ---------------------------------------- 2/2 [pyspark]\n",
      "\n",
      "Successfully installed py4j-0.10.9.9 pyspark-4.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36dbc689-96d7-4f42-95a0-db04f85b6952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\ankus\\anaconda3\\lib\\site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba667c16-b397-468f-8ac6-3bbe4af78dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom pyspark.sql import SparkSession\\n\\n# Step 1: Initialize SparkSession\\nspark = SparkSession.builder             .appName(\\'Read File Example\\')             .getOrCreate()\\n\\n# Step 2: Read Data\\ndf = spark.read         .format(\"<file_format>\")         .option(\"<option_name>\", \"option_value\")         .load(\"<path_to_file_or_folder\")\\n\\n# Step 3: Inspect Data\\ndf.show(5)\\ndf.printSchema()\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General Format for reading a file in PySpark\n",
    "\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Step 1: Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "            .appName('Read File Example') \\\n",
    "            .getOrCreate()\n",
    "\n",
    "# Step 2: Read Data\n",
    "df = spark.read \\\n",
    "        .format(\"<file_format>\") \\\n",
    "        .option(\"<option_name>\", \"option_value\") \\\n",
    "        .load(\"<path_to_file_or_folder\")\n",
    "\n",
    "# Step 3: Inspect Data\n",
    "df.show(5)\n",
    "df.printSchema()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30c11bbd-acf1-4480-b72d-db72e66f47d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                .appName(\"Reading Files\") \\\n",
    "                .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7e72bdd-5187-4fa7-9a12-ff5a2b4e9fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a csv file\n",
    "\n",
    "# df = spark.read \\\n",
    "#         .format('csv') \\\n",
    "#         .option('header', 'true') \\\n",
    "#         .option('inferSchema', 'true') \\\n",
    "#         .load(\"/datasets/customers.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eebb8c06-ed0f-4d34-9a17-b13e31b13695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a Parquet File\n",
    "\n",
    "# df = spark.read \\\n",
    "#         .format('parquet') \\\n",
    "#         .load(\"/datasets/sales.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91b8e872-617e-4a6e-b640-e631f452e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a JSON File\n",
    "\n",
    "# df = spark.read \\\n",
    "#         .format(\"json\") \\\n",
    "#         .load(\"datasets/events.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e944d514-c601-4beb-a9cc-9df2ac92ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading from Hive table using spark.read.table()\n",
    "\n",
    "# df_hive = spark.read.table(\"default.customers\")\n",
    "\n",
    "# df_hive.show(5)\n",
    "# df_hive.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69138f01-959d-490a-8a0b-9ebc501d43f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading from Hive table using spark sql\n",
    "\n",
    "# df_hive_sql = spark.sql(\"\"\"\n",
    "#     select\n",
    "#         id,\n",
    "#         name,\n",
    "#         email\n",
    "#     from \n",
    "#         default.customers\n",
    "#     where\n",
    "#         country = 'US'\n",
    "# \"\"\")\n",
    "\n",
    "# df_hive_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce151a7a-4537-4d1e-b954-e7c1561646e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading from a Database (JDBC)\n",
    "# One important thing is that we can connect to JDBC while creating spark session, otherwise we can connect later while \n",
    "\n",
    "# df = spark.read \\\n",
    "#         .format(\"jdbc\") \\\n",
    "#         .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\") \\  # jdbc connection string\n",
    "#         .option(\"dbtable\", \"public.customers\") \\\n",
    "#         .option(\"user\", \"postgres\") \\\n",
    "#         .option(\"password\", \"postgres\") \\\n",
    "#         .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e76c3c78-890e-4f3b-968e-fa91f710e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general pattern is .read.format().options().load() that covers almost everything\n",
    "\n",
    "# or there are some shortcut methods for reading csv parquet etc\n",
    "\n",
    "# df_csv = spark.read.csv(path, header=True, inferSchema=True)\n",
    "# df_json = spark.read.json(path)\n",
    "# df_parquet = spark.read.parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e828ade-6009-46d7-8704-18ec32860924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom pyspark.sql import SparkSession\\n\\n# step 1: init spark session -- pyspark.builder api\\nspark = SparkSession.builder             .appName(\"Writing File\")             .getOrCreate()\\n\\n# step 2: write data (dataFrame data)\\n\\ndf.write     .format(\"<file_format>\")     .option(\"<option_mode>\", \"<option_value>\") \\\\  # used to configure writing options .option(\"header\", \"true\") or .option(\"compression\", \"gzip\")\\n    .mode(\"<save_mode>\") \\\\  # save behavior when output exists (\\'overwrite\\', \\'append\\', \\'ignore\\', \\'error\\') \\n    .save(\"<path_to_output_file_or_folder>\")\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# writing files in PySpark\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# step 1: init spark session -- pyspark.builder api\n",
    "spark = SparkSession.builder \\\n",
    "            .appName(\"Writing File\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "# step 2: write data (dataFrame data)\n",
    "\n",
    "df.write \\\n",
    "    .format(\"<file_format>\") \\\n",
    "    .option(\"<option_mode>\", \"<option_value>\") \\  # used to configure writing options .option(\"header\", \"true\") or .option(\"compression\", \"gzip\")\n",
    "    .mode(\"<save_mode>\") \\  # save behavior when output exists ('overwrite', 'append', 'ignore', 'error') \n",
    "    .save(\"<path_to_output_file_or_folder>\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df4e9f72-50e5-47be-abcc-0c0a299010f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save mode\n",
    "\n",
    "# overwrite - replace existing data\n",
    "# append - adds data to existing files or tables\n",
    "# ignore - skips writing if the destination exists/ if files are already exists\n",
    "# error or errorIfExists - fails if the destination exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e0c885b-d127-481f-bf71-fcc1b1ab4472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # writing in csv file\n",
    "\n",
    "# df.write \\\n",
    "#     .format('csv') \\  # in what format we want to write\n",
    "#     .option('header', 'true') \\  # this includes header otherwise treat this a row\n",
    "#     .mode('overwrite') \\\n",
    "#     .save('/output/customers_csv')\n",
    "\n",
    "# # writing in parquet file\n",
    "# df.write \\\n",
    "#     .format('parquet') \\\n",
    "#     .mode('overwrite') \\\n",
    "#     .save('/output/sales_parquet')\n",
    "\n",
    "# # writing in json file\n",
    "# df.write \\\n",
    "#     .format('json') \\\n",
    "#     .mode('overwrite') \\\n",
    "#     .save('/output/events_json')\n",
    "\n",
    "# # writing to a HIVE table\n",
    "# df.write \\\n",
    "#     .mode('overwrite') \\\n",
    "#     .saveAsTable('default.customers_copy')\n",
    "\n",
    "# # writing to a DataBase(JDBC)\n",
    "# df.write \\\n",
    "#     .format('jdbc') \\\n",
    "#     .option(\"url\", \"jdbc:postgresql://loacalhost:5432/mydb\") \\\n",
    "#     .option(\"dbtable\", \"public.customers\") \\\n",
    "#     .option(\"user\", \"postgres\") \\\n",
    "#     .option(\"password\", \"postgres\") \\\n",
    "#     .mode(\"append\") \\\n",
    "#     .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d474f1d3-7149-4ac7-90f2-09d00f23d26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortcut for common formats\n",
    "\n",
    "# df.write.csv(path=\"/outputs/customers\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# df.write.json(path=\"/output/events\", mode=\"overwrite\")\n",
    "\n",
    "# df.write.parquet(path=\"/output/sales\", mode=\"overwrite\")\n",
    "\n",
    "# common format for write - df.write.format().options().mode().save()\n",
    "# df.write.format().options().mode().save() --> Flexibility & Reusability\n",
    "# This pattern is ideal for building reusable ETL pipelines, because you can easily switch formats, destinations or save modes without changing the main logic. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
